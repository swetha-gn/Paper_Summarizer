# utils/evaluator.py
from typing import List, Dict
import pandas as pd
from bert_score import score
from utils.logger import get_logger
import torch

logger = get_logger(__name__)

class SummaryEvaluator:
    def __init__(self):
        """Initialize the evaluator with BERTScore."""
        self.metrics = ['precision', 'recall', 'f1']
        # Use GPU if available
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    def evaluate_summary(self, generated_summary: str, reference_summary: str) -> Dict:
        """
        Evaluate a single summary using BERTScore.
        
        Args:
            generated_summary: Summary generated by our system
            reference_summary: Ground truth summary (e.g., paper abstract)
            
        Returns:
            Dict containing BERTScore metrics
        """
        try:
            P, R, F1 = score(
                cands=[generated_summary],
                refs=[reference_summary],
                lang='en',
                model_type='microsoft/deberta-xlarge-mnli',  # High performance model
                device=self.device
            )
            
            results = {
                'bertscore_precision': P.item(),
                'bertscore_recall': R.item(),
                'bertscore_f1': F1.item()
            }
            
            return results
            
        except Exception as e:
            logger.error(f"Error calculating BERTScore: {str(e)}")
            return {}

    def evaluate_batch(self, summaries: List[Dict]) -> pd.DataFrame:
        """
        Evaluate multiple summaries and return aggregated scores.
        
        Args:
            summaries: List of dictionaries containing generated and reference summaries
            
        Returns:
            DataFrame with BERTScore metrics for each summary
        """
        try:
            all_scores = []
            generated_summaries = []
            reference_summaries = []
            titles = []
            
            for summary in summaries:
                if 'generated_summary' in summary and 'reference_summary' in summary:
                    generated_summaries.append(summary['generated_summary'])
                    reference_summaries.append(summary['reference_summary'])
                    titles.append(summary.get('title', 'Unknown'))
            
            if generated_summaries:
                # Batch computation of BERTScore
                P, R, F1 = score(
                    cands=generated_summaries,
                    refs=reference_summaries,
                    lang='en',
                    model_type='microsoft/deberta-xlarge-mnli',
                    device=self.device
                )
                
                # Create scores for each summary
                for i in range(len(titles)):
                    scores = {
                        'title': titles[i],
                        'bertscore_precision': P[i].item(),
                        'bertscore_recall': R[i].item(),
                        'bertscore_f1': F1[i].item()
                    }
                    all_scores.append(scores)
            
            # Create DataFrame
            df = pd.DataFrame(all_scores)
            
            # Add average scores
            if not df.empty:
                averages = df.select_dtypes(include=['float64']).mean()
                averages['title'] = 'AVERAGE'
                df = pd.concat([df, pd.DataFrame([averages])], ignore_index=True)
            
            return df
            
        except Exception as e:
            logger.error(f"Error in batch evaluation: {str(e)}")
            return pd.DataFrame()

    def save_evaluation(self, df: pd.DataFrame, output_path: str):
        """Save evaluation results to CSV."""
        try:
            df.to_csv(output_path, index=False)
            logger.info(f"Saved evaluation results to {output_path}")
        except Exception as e:
            logger.error(f"Error saving evaluation results: {str(e)}")